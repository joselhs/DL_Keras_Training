{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bostong Housing Price with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, layers\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "Vamos a normalizar los datos ya que siendo todas las columnas numéricas, los valores entre los que oscilan son muy diferentes, esto puede causar que columnas con valores más altos tengan más peso que otras de valores más pequeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model\n",
    "En este caso vamos a crear una función que cree nuestro model, de este modo no tenemos que reproducir el mismo código tantas veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1], )))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Como nuestros datos tienen un tamaño pequeño, tendríamos que validar con un conjunto de datos que podría ser insuficiente, ya que mostraría dependiendo de los datos que usaramos podrían cambiar mucho los resultados. Para evitar esto usaremos k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('Processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples : (i+1) * num_val_samples]\n",
    "    val_targets = train_labels[i * num_val_samples : (i+1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[ : i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples : ]],\n",
    "        axis = 0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate( \n",
    "        [train_labels[:i * num_val_samples],\n",
    "         train_labels[(i + 1) * num_val_samples:]], \n",
    "        axis=0)\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, \n",
    "              partial_train_targets, \n",
    "              epochs=num_epochs, \n",
    "              batch_size=1, \n",
    "              verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 500\n",
    "all_mae_histories = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('Processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples : (i+1) * num_val_samples]\n",
    "    val_targets = train_labels[i * num_val_samples : (i+1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[ : i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples : ]],\n",
    "        axis = 0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate( \n",
    "        [train_labels[:i * num_val_samples],\n",
    "         train_labels[(i + 1) * num_val_samples:]], \n",
    "        axis=0)\n",
    "    \n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, \n",
    "                        partial_train_targets, \n",
    "                        epochs=num_epochs,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        batch_size=1, \n",
    "                        verbose=0)\n",
    "    \n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ser dificil ver la curva debido a la alta varianza que hay vamos a realizar lo siguiente:\n",
    "\n",
    "- Eliminar los 10 primeros puntos, ya que están a una escala diferente que el resto\n",
    "- Sustituir cada punto por una media exponencial movil de los puntos anteriores, para de este modo ver una curva suavizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_mae_history = smooth_curve(average_mae_history[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, \n",
    "          train_labels,\n",
    "          epochs=50,\n",
    "          batch_size=16,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
